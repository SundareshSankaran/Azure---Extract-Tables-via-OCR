{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from azure.ai.documentintelligence.models._models import AnalyzeResult\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.environ.get('OCR_KEY')\n",
    "endpoint = os.environ.get('OCR_ENDPOINT')\n",
    "credential = AzureKeyCredential(key)\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint, credential, api_version='2023-10-31-preview')\n",
    "doc_path ='../data/german-handwriting-sample.jpeg'\n",
    "\n",
    "with open(doc_path, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", \n",
    "        analyze_request=f, \n",
    "        content_type=\"application/octet-stream\", \n",
    "    )\n",
    "r_cloud = poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "service_url = 'http://localhost:5000/'\n",
    "\n",
    "# Define the URL and file path\n",
    "url = f\"{service_url}formrecognizer/documentModels/prebuilt-read:syncAnalyze?api-version=2022-08-31\"\n",
    "file_path = \"../data/handwritten-form.jpg\"\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'Content-Type': 'application/octet-stream',\n",
    "}\n",
    "\n",
    "# Open the file and send the POST request\n",
    "with open(file_path, 'rb') as file:\n",
    "    response = requests.post(url, headers=headers, data=file)\n",
    "\n",
    "r_local = json.loads(response.text)\n",
    "r_local = r_local['analyzeResult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud keys: dict_keys(['apiVersion', 'modelId', 'stringIndexType', 'content', 'pages', 'paragraphs', 'styles', 'contentFormat'])\n",
      "local keys: dict_keys(['apiVersion', 'modelId', 'stringIndexType', 'content', 'pages', 'paragraphs', 'styles', 'languages'])\n"
     ]
    }
   ],
   "source": [
    "print(f'cloud keys: {r_cloud.keys()}')\n",
    "print(f'local keys: {r_local.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud keys: dict_keys(['pageNumber', 'angle', 'width', 'height', 'unit', 'words', 'lines', 'spans'])\n",
      "local keys: dict_keys(['pageNumber', 'angle', 'width', 'height', 'unit', 'words', 'lines', 'spans', 'kind'])\n",
      "cloud keys: [{'confidence': 1, 'spans': [{'offset': 34, 'length': 6}, {'offset': 56, 'length': 20}, {'offset': 303, 'length': 32}, {'offset': 339, 'length': 2}, {'offset': 346, 'length': 3}, {'offset': 354, 'length': 3}, {'offset': 374, 'length': 5}, {'offset': 387, 'length': 6}, {'offset': 399, 'length': 4}, {'offset': 410, 'length': 5}, {'offset': 423, 'length': 6}, {'offset': 433, 'length': 2}, {'offset': 441, 'length': 4}, {'offset': 452, 'length': 5}, {'offset': 465, 'length': 6}, {'offset': 475, 'length': 2}, {'offset': 482, 'length': 3}, {'offset': 492, 'length': 5}, {'offset': 505, 'length': 9}, {'offset': 519, 'length': 3}, {'offset': 543, 'length': 6}, {'offset': 553, 'length': 2}, {'offset': 560, 'length': 3}, {'offset': 569, 'length': 4}, {'offset': 580, 'length': 5}, {'offset': 613, 'length': 28}, {'offset': 669, 'length': 26}, {'offset': 1075, 'length': 329}], 'isHandwritten': True}, {'confidence': 0.95, 'spans': [{'offset': 363, 'length': 4}], 'isHandwritten': True}, {'confidence': 0.8, 'spans': [{'offset': 528, 'length': 4}], 'isHandwritten': True}]\n",
      "local keys: [{'confidence': 1, 'spans': [{'offset': 34, 'length': 6}, {'offset': 56, 'length': 20}, {'offset': 303, 'length': 10}, {'offset': 317, 'length': 2}, {'offset': 329, 'length': 21}, {'offset': 364, 'length': 10}, {'offset': 380, 'length': 5}, {'offset': 395, 'length': 4}, {'offset': 406, 'length': 5}, {'offset': 419, 'length': 6}, {'offset': 429, 'length': 3}, {'offset': 444, 'length': 5}, {'offset': 457, 'length': 6}, {'offset': 467, 'length': 2}, {'offset': 474, 'length': 11}, {'offset': 492, 'length': 5}, {'offset': 505, 'length': 6}, {'offset': 519, 'length': 3}, {'offset': 533, 'length': 2}, {'offset': 543, 'length': 6}, {'offset': 553, 'length': 2}, {'offset': 565, 'length': 3}, {'offset': 575, 'length': 10}, {'offset': 613, 'length': 46}, {'offset': 687, 'length': 40}, {'offset': 1107, 'length': 328}], 'isHandwritten': True}, {'confidence': 0.95, 'spans': [{'offset': 375, 'length': 4}, {'offset': 528, 'length': 4}], 'isHandwritten': True}]\n"
     ]
    }
   ],
   "source": [
    "print(f'cloud keys: {r_cloud[\"pages\"][0].keys()}')\n",
    "print(f'local keys: {r_local[\"pages\"][0].keys()}')\n",
    "\n",
    "print(f'cloud keys: {r_cloud[\"styles\"]}')\n",
    "print(f'local keys: {r_local[\"styles\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud keys: dict_keys(['content', 'polygon', 'confidence', 'span'])\n",
      "local keys: dict_keys(['content', 'polygon', 'confidence', 'span'])\n"
     ]
    }
   ],
   "source": [
    "print(f'cloud keys: {r_cloud[\"pages\"][0][\"words\"][0].keys()}')\n",
    "print(f'local keys: {r_local[\"pages\"][0][\"words\"][0].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud keys: [187, 15, 313, 15, 313, 34, 187, 33]\n",
      "local keys: [186, 13, 315, 14, 314, 37, 186, 35]\n",
      "cloud keys: HANDWRITING\n",
      "local keys: HANDWRITING\n",
      "cloud keys: 0.993\n",
      "local keys: 0.991\n",
      "cloud keys: {'offset': 0, 'length': 11}\n",
      "local keys: {'offset': 0, 'length': 11}\n"
     ]
    }
   ],
   "source": [
    "print(f'cloud keys: {r_cloud[\"pages\"][0][\"words\"][0][\"polygon\"]}')\n",
    "print(f'local keys: {r_local[\"pages\"][0][\"words\"][0][\"polygon\"]}')\n",
    "\n",
    "print(f'cloud keys: {r_cloud[\"pages\"][0][\"words\"][0][\"content\"]}')\n",
    "print(f'local keys: {r_local[\"pages\"][0][\"words\"][0][\"content\"]}')\n",
    "\n",
    "print(f'cloud keys: {r_cloud[\"pages\"][0][\"words\"][0][\"confidence\"]}')\n",
    "print(f'local keys: {r_local[\"pages\"][0][\"words\"][0][\"confidence\"]}')\n",
    "\n",
    "print(f'cloud keys: {r_cloud[\"pages\"][0][\"words\"][0][\"span\"]}')\n",
    "print(f'local keys: {r_local[\"pages\"][0][\"words\"][0][\"span\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parsing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>text</th>\n",
       "      <th>role</th>\n",
       "      <th>bb_x1</th>\n",
       "      <th>bb_y1</th>\n",
       "      <th>bb_x2</th>\n",
       "      <th>bb_y2</th>\n",
       "      <th>bb_x3</th>\n",
       "      <th>bb_y3</th>\n",
       "      <th>bb_x4</th>\n",
       "      <th>bb_y4</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Liebe Handschrift,</td>\n",
       "      <td></td>\n",
       "      <td>89</td>\n",
       "      <td>41</td>\n",
       "      <td>339</td>\n",
       "      <td>36</td>\n",
       "      <td>340</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ich mag Dich nicht besonders. Jeder Drittkläss...</td>\n",
       "      <td></td>\n",
       "      <td>81</td>\n",
       "      <td>124</td>\n",
       "      <td>917</td>\n",
       "      <td>108</td>\n",
       "      <td>923</td>\n",
       "      <td>399</td>\n",
       "      <td>86</td>\n",
       "      <td>415</td>\n",
       "      <td>19</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Aber brauche ich Dich noch in einem Alltag, in...</td>\n",
       "      <td></td>\n",
       "      <td>87</td>\n",
       "      <td>423</td>\n",
       "      <td>946</td>\n",
       "      <td>438</td>\n",
       "      <td>942</td>\n",
       "      <td>667</td>\n",
       "      <td>83</td>\n",
       "      <td>652</td>\n",
       "      <td>392</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Du bildest unsere Persönlichkeit ab. Trotzdem ...</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td>696</td>\n",
       "      <td>934</td>\n",
       "      <td>701</td>\n",
       "      <td>932</td>\n",
       "      <td>1056</td>\n",
       "      <td>84</td>\n",
       "      <td>1051</td>\n",
       "      <td>712</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Schriftliche grüße ,</td>\n",
       "      <td></td>\n",
       "      <td>94</td>\n",
       "      <td>1081</td>\n",
       "      <td>338</td>\n",
       "      <td>1087</td>\n",
       "      <td>337</td>\n",
       "      <td>1128</td>\n",
       "      <td>93</td>\n",
       "      <td>1122</td>\n",
       "      <td>1145</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page  paragraph                                               text role  \\\n",
       "0     1          0                                 Liebe Handschrift,        \n",
       "1     1          1  ich mag Dich nicht besonders. Jeder Drittkläss...        \n",
       "2     1          2  Aber brauche ich Dich noch in einem Alltag, in...        \n",
       "3     1          3  Du bildest unsere Persönlichkeit ab. Trotzdem ...        \n",
       "4     1          4                               Schriftliche grüße ,        \n",
       "\n",
       "   bb_x1  bb_y1  bb_x2  bb_y2  bb_x3  bb_y3  bb_x4  bb_y4  offset  length  \n",
       "0     89     41    339     36    340     78     90     83       0      18  \n",
       "1     81    124    917    108    923    399     86    415      19     372  \n",
       "2     87    423    946    438    942    667     83    652     392     319  \n",
       "3     86    696    934    701    932   1056     84   1051     712     432  \n",
       "4     94   1081    338   1087    337   1128     93   1122    1145      20  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_granularity = 'paragraph'\n",
    "model_id = 'prebuilt-read'\n",
    "\n",
    "def parse_ocr_result(result) -> pd.DataFrame:\n",
    "    text_granularity = 'PARAGRPAH'\n",
    "    model_id = 'prebuilt-layout'\n",
    "    parsed_result = None\n",
    "    # azure doesn't provide results on page level natively\n",
    "    level = text_granularity\n",
    "    if (level.upper() == 'PAGE'):\n",
    "        text_granularity = \"LINE\"\n",
    "    else:\n",
    "        text_granularity = level.upper()\n",
    "\n",
    "    for page in result['pages']:\n",
    "        try:\n",
    "            contains_handwriting = result.styles[0].is_handwritten\n",
    "        except:\n",
    "            contains_handwriting = False\n",
    "\n",
    "        ocr_data = []\n",
    "        \n",
    "        # to calculate the average confidence\n",
    "        if text_granularity != \"WORD\":\n",
    "            word_confidences = [word['confidence'] for word in page['words']]\n",
    "            total_confidence = sum(word_confidences)\n",
    "            total_words = len(word_confidences)\n",
    "            average_confidence = total_confidence / total_words if total_words > 0 else 0\n",
    "            \n",
    "        # extraction of (natively provided) results \n",
    "        if text_granularity == \"PARAGRPAH\":\n",
    "            for paragraph_idx, paragraph in enumerate(result['paragraphs']):\n",
    "                x1, y1, x2, y2, x3, y3, x4, y4 = paragraph['boundingRegions'][0]['polygon']\n",
    "\n",
    "                try: \n",
    "                    role = paragraph['role']\n",
    "                except:\n",
    "                    role = ''\n",
    "\n",
    "                paragrpah_info = {\n",
    "                    \"page\": paragraph['boundingRegions'][0]['pageNumber'],\n",
    "                    \"paragraph\": paragraph_idx,\n",
    "                    \"text\": paragraph['content'],\n",
    "                    \"role\": role,\n",
    "                    \"bb_x1\": x1,\n",
    "                    \"bb_y1\": y1,\n",
    "                    \"bb_x2\": x2,\n",
    "                    \"bb_y2\": y2,\n",
    "                    \"bb_x3\": x3,\n",
    "                    \"bb_y3\": y3,\n",
    "                    \"bb_x4\": x4,\n",
    "                    \"bb_y4\": y4,\n",
    "                    \"offset\": paragraph['spans'][0]['offset'],\n",
    "                    \"length\": paragraph['spans'][0]['length'],\n",
    "                    \n",
    "                }\n",
    "                \n",
    "                ocr_data.append(paragrpah_info)\n",
    "\n",
    "        elif text_granularity == \"LINE\":\n",
    "            for line_idx, line in enumerate(page['lines']):\n",
    "                x1, y1, x2, y2, x3, y3, x4, y4 = line['polygon']\n",
    "\n",
    "                line_info = {\n",
    "                    \"page\": page['pageNumber'],\n",
    "                    \"line\": line_idx,\n",
    "                    \"text\": line['content'],\n",
    "                    \"bb_x1\": x1,\n",
    "                    \"bb_y1\": y1,\n",
    "                    \"bb_x2\": x2,\n",
    "                    \"bb_y2\": y2,\n",
    "                    \"bb_x3\": x3,\n",
    "                    \"bb_y3\": y3,\n",
    "                    \"bb_x4\": x4,\n",
    "                    \"bb_y4\": y4,\n",
    "                    \"offset\": line['spans'][0]['offset'],\n",
    "                    \"length\": line['spans'][0]['length'],\n",
    "                }\n",
    "                \n",
    "                ocr_data.append(line_info)\n",
    "\n",
    "        elif text_granularity == \"WORD\":\n",
    "            for word in page.words:\n",
    "                x1, y1, x2, y2, x3, y3, x4, y4 = word['polygon']\n",
    "\n",
    "                word_info = {\n",
    "                    \"page\": page['pageNumber'],\n",
    "                    \"text\": word['content'],\n",
    "                    \"confidence\": word['confidence'],\n",
    "                    \"bb_x1\": x1,\n",
    "                    \"bb_y1\": y1,\n",
    "                    \"bb_x2\": x2,\n",
    "                    \"bb_y2\": y2,\n",
    "                    \"bb_x3\": x3,\n",
    "                    \"bb_y3\": y3,\n",
    "                    \"bb_x4\": x4,\n",
    "                    \"bb_y4\": y4,\n",
    "                    \"offset\": word['span']['offset'],\n",
    "                    \"length\": word['span']['length'],\n",
    "                    }\n",
    "                \n",
    "                ocr_data.append(word_info)\n",
    "        \n",
    "        df = pd.DataFrame(ocr_data)\n",
    "\n",
    "        # in case texts should be aggreagted on page level\n",
    "        if level.upper() == \"PAGE\":\n",
    "            ocr_data = []\n",
    "            page_info = {\n",
    "                    \"page\": page['pageNumber'],\n",
    "                    \"text\": \"\\n \".join(df['text']),\n",
    "                    \"avg_confidence\": average_confidence,\n",
    "                    \"contains_handwriting\": contains_handwriting,\n",
    "                    \"bb_x1\": df[\"bb_x1\"].min(),\n",
    "                    \"bb_y1\": df[\"bb_y1\"].min(),\n",
    "                    \"bb_x2\": df[\"bb_x2\"].max(),\n",
    "                    \"bb_y2\": df[\"bb_y2\"].min(),\n",
    "                    \"bb_x3\": df[\"bb_x3\"].max(),\n",
    "                    \"bb_y3\": df[\"bb_y3\"].max(),\n",
    "                    \"bb_x4\": df[\"bb_x4\"].min(),\n",
    "                    \"bb_x4\": df[\"bb_x4\"].max(),\n",
    "                    }\n",
    "            ocr_data.append(page_info)\n",
    "            \n",
    "            df = pd.DataFrame(ocr_data)\n",
    "\n",
    "    if model_id == 'prebuilt-read' and text_granularity.upper() == 'PARAGRAPH': # 'read' model doesn't provide semantic role, only 'layout' does\n",
    "        parsed_result = parsed_result.drop(columns=['role'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = parse_ocr_result(r_cloud)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['spans', 'boundingRegions', 'content'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cloud['paragraphs'][3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.993, 0.997, 0.986, 0.865, 0.984, 0.992, 0.984, 0.996, 0.994, 0.918, 0.981, 0.951, 0.97, 0.992, 0.992, 0.993, 0.994, 0.991, 0.991, 0.994, 0.997, 0.998, 0.998, 0.994, 0.993, 0.993, 0.998, 0.993, 0.994, 0.993, 0.997, 0.962, 0.993, 0.992, 0.998, 0.994, 0.994, 0.999, 0.999, 0.994, 0.991, 0.994, 0.988, 0.977, 0.989, 0.994, 0.992, 0.995, 0.951, 0.991, 0.945, 0.998, 0.231, 0.994, 0.924, 0.99, 0.981, 0.995, 0.852, 0.994, 0.99, 0.975, 0.951, 0.997, 0.967, 0.994, 0.974, 0.993, 0.999, 0.991, 0.995, 0.993, 0.997, 0.993, 0.994, 0.998, 0.996, 0.998, 0.997, 0.995, 0.99, 0.994, 0.918, 0.924, 0.998, 0.984, 0.988, 0.844, 0.972, 0.994, 0.994, 0.999, 0.999, 0.998, 0.988, 0.981, 0.963, 0.997, 0.994, 0.878, 0.154, 0.335, 0.859, 0.891, 0.99, 0.993, 0.998, 0.992, 0.988, 0.999, 0.998, 0.976, 0.988, 0.989, 0.998, 0.994, 0.998, 0.998, 0.994, 0.993, 0.99, 0.991, 0.997, 0.993, 0.991, 0.993, 0.989, 0.994, 0.993, 0.994, 0.993, 0.99, 0.959, 0.993, 0.997, 0.993, 0.989, 0.975, 0.994, 0.998, 0.992, 0.988, 0.998, 0.991, 0.998, 0.993, 0.998, 0.994, 0.991, 0.959, 0.998, 0.997, 0.932, 0.991, 0.994, 0.998, 0.993, 0.991, 0.993, 0.996, 0.998, 0.986, 0.993, 0.998, 0.985, 0.894, 0.995, 0.845, 0.99, 0.992, 0.717, 0.951, 0.957, 0.924, 0.994, 0.989, 0.985, 0.915, 0.79, 0.963, 0.918, 0.699, 0.961, 0.89, 0.974, 0.992, 0.993, 0.985, 0.951, 0.806, 0.992, 0.949, 0.852, 0.994, 0.918, 0.987, 0.924, 0.996, 0.601, 0.997, 0.937, 0.972, 0.997, 0.924, 0.827, 0.995, 0.99, 0.991, 0.957, 0.984, 0.938, 0.975, 0.994, 0.985, 0.982, 0.996, 0.924]\n"
     ]
    }
   ],
   "source": [
    "for page in r_cloud['pages']:\n",
    "\n",
    "    word_confidences = [word['confidence'] for word in page.words]\n",
    "    print(word_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
